import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from scipy.stats import norm
import time
import os # For saving model


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


sigma = 0.2     
r = 0.05      
K = 100.0     
T = 1.0       
S_min = 0.01  
S_max = 200.0   


MODEL_SAVE_DIR = "pinn_models"
MODEL_FILENAME = "bs_pinn_stage2.pth"
MODEL_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, MODEL_FILENAME)
LOSS_DATA_PATH = os.path.join(MODEL_SAVE_DIR, "bs_pinn_stage2_losses.npz")


os.makedirs(MODEL_SAVE_DIR, exist_ok=True)


# --- Analytical Solution (for validation) ---
def black_scholes_call_numpy(S, t, sigma, r, K=100, T=1):
    tau = T - t
    price = np.zeros_like(S)
    is_scalar_S = np.isscalar(S)
    is_scalar_t = np.isscalar(t)

    if is_scalar_S and is_scalar_t:
        S_arr, t_arr = np.array([S]), np.array([t])
    elif is_scalar_S:
        S_arr = np.full_like(t, S)
        t_arr = t
    elif is_scalar_t:
        S_arr = S
        t_arr = np.full_like(S, t)
    else: 
        S_arr, t_arr = S, t

    tau_arr = T - t_arr
    price = np.zeros_like(S_arr)

    idx_pos_tau = tau_arr > 1e-8
    S_pos = S_arr[idx_pos_tau]
    tau_pos = tau_arr[idx_pos_tau]

    if S_pos.size > 0: 
        S_pos = np.maximum(S_pos, 1e-10) 
        d1 = (np.log(S_pos/K) + (r + 0.5 * sigma**2) * tau_pos) / (sigma * np.sqrt(tau_pos))
        d2 = d1 - sigma * np.sqrt(tau_pos)
        price[idx_pos_tau] = S_pos * norm.cdf(d1) - K * np.exp(-r * tau_pos) * norm.cdf(d2)

    idx_zero_tau = ~idx_pos_tau
    price[idx_zero_tau] = np.maximum(S_arr[idx_zero_tau] - K, 0.0)

    if is_scalar_S and is_scalar_t:
        return price.item()
    else:
        return price

class PINN(nn.Module):
    def __init__(self, hidden_dim=128): 
        super(PINN, self).__init__()
        # Input: (S, t), Output: u(S, t)
        self.net = nn.Sequential(
            nn.Linear(2, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim), 
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
        # Initialization
        for m in self.net.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)

    def forward(self, S, t):
        X = torch.cat([S.reshape(-1, 1), t.reshape(-1, 1)], dim=1)
        return self.net(X)
    

def pde_loss(model, S, t, sigma, r):
    """ Calculates the PDE residual loss """
    S.requires_grad_(True)
    t.requires_grad_(True)

    u = model(S, t)

    # First derivatives
    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]
    u_S = torch.autograd.grad(u, S, grad_outputs=torch.ones_like(u), create_graph=True)[0]

    # Second derivative
    u_SS = torch.autograd.grad(u_S, S, grad_outputs=torch.ones_like(u_S), create_graph=True)[0]

    # PDE Residual ( L(u) = u_t + r*S*u_S + 0.5*sigma^2*S^2*u_SS - r*u )
    residual = u_t + r * S * u_S + 0.5 * (sigma**2) * (S**2) * u_SS - r * u

    # Mean Squared Error of the residual
    loss_f = torch.mean(residual**2)
    return loss_f

def boundary_loss(model, n_bc, device):
    """ Calculates the boundary condition losses """
    # BC 1: u(S_min, t) = 0
    t_bc1 = torch.rand(n_bc, 1, device=device) * T  
    S_bc1 = torch.full_like(t_bc1, S_min)       
    u_bc1 = model(S_bc1, t_bc1)
    loss_bc1 = torch.mean(u_bc1**2)

    # BC 2: du/dS(S_max, t) = 1
    t_bc2 = torch.rand(n_bc, 1, device=device) * T  # t in [0, T]
    S_bc2 = torch.full_like(t_bc2, S_max)        # S = S_max
    S_bc2.requires_grad_(True) 

    u_bc2 = model(S_bc2, t_bc2)
    u_S_bc2 = torch.autograd.grad(u_bc2.sum(), S_bc2, create_graph=True)[0] # Use .sum() for scalar output grad
    loss_bc2 = torch.mean((u_S_bc2 - 1.0)**2)

    return loss_bc1 + loss_bc2

def terminal_loss(model, n_ic, device):
    """ Calculates the terminal condition loss (at t=T) """
    S_ic = torch.rand(n_ic, 1, device=device) * (S_max - S_min) + S_min # S in [S_min, S_max]
    t_ic = torch.full_like(S_ic, T)                         # t = T
    u_ic = model(S_ic, t_ic)

    # Target payoff: max(S - K, 0)
    payoff = torch.maximum(S_ic - K, torch.zeros_like(S_ic))
    loss_ic = torch.mean((u_ic - payoff)**2)
    return loss_ic


pinn_model = PINN(hidden_dim=128).to(device) 

optimizer = optim.AdamW(pinn_model.parameters(), lr=1e-3, weight_decay=1e-5)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=1000, verbose=True)

# Number of points for each loss component
n_pde = 10000  # Collocation points for PDE residual
n_bc = 1000   # Points for boundary conditions
n_ic = 1000   # Points for terminal condition

# Loss weights (Crucial for tuning!)
w_pde = 1.0
w_bc = 1.0   # Might need to increase if they learn slowly
w_ic = 1.0   # Might need to increase if payoff isn't matched well

num_epochs = 1000 
log_frequency = 500

epochs_recorded = []
losses = {'total': [], 'pde': [], 'bc': [], 'ic': []}

print("Starting Training:")
start_time = time.time()

for epoch in range(num_epochs):
    pinn_model.train()
    optimizer.zero_grad()

    # Sample points for PDE loss (collocation points in the interior)
    S_pde = torch.rand(n_pde, 1, device=device) * (S_max - S_min - 2*1e-5) + S_min + 1e-5
    t_pde = torch.rand(n_pde, 1, device=device) * T # t != T exactly

    # Calculate losses
    loss_f = pde_loss(pinn_model, S_pde, t_pde, sigma, r)
    loss_b = boundary_loss(pinn_model, n_bc, device)
    loss_i = terminal_loss(pinn_model, n_ic, device)

    total_loss = w_pde * loss_f + w_bc * loss_b + w_ic * loss_i

    # Backpropagation
    total_loss.backward()

    # Optional: Gradient Clipping (can help stability)
    torch.nn.utils.clip_grad_norm_(pinn_model.parameters(), max_norm=1.0)

    optimizer.step()

    scheduler.step(total_loss)

    # Logging
    if (epoch + 1) % log_frequency == 0 or epoch == 0:
        epochs_recorded.append(epoch + 1)
        losses['total'].append(total_loss.item())
        losses['pde'].append(loss_f.item())
        losses['bc'].append(loss_b.item())
        losses['ic'].append(loss_i.item())
        print(f"Epoch [{epoch+1}/{num_epochs}], "
              f"Loss: {total_loss.item():.4e}, "
              f"PDE: {loss_f.item():.4e}, "
              f"BC: {loss_b.item():.4e}, "
              f"IC: {loss_i.item():.4e}, "
              f"LR: {optimizer.param_groups[0]['lr']:.2e}")

end_time = time.time()
training_time = end_time - start_time
print(f"\nTraining finished in {training_time:.2f} seconds.")

# Convert recorded epochs to numpy array
epochs_recorded_np = np.array(epochs_recorded)

# Save losses and epochs
np.savez(LOSS_DATA_PATH,
         epochs=epochs_recorded_np,
         total=np.array(losses['total']),
         pde=np.array(losses['pde']),
         bc=np.array(losses['bc']),
         ic=np.array(losses['ic']),
         training_time=training_time,
         final_lr=optimizer.param_groups[0]['lr']) # Save final LR

print(f"Loss data saved to {LOSS_DATA_PATH}")

# Save the trained model state dictionary
torch.save(pinn_model.state_dict(), MODEL_SAVE_PATH)
print(f"Model state dictionary saved to {MODEL_SAVE_PATH}")

# --- Load Model ---
pinn_model_loaded = PINN(hidden_dim=128).to(device) # Match architecture used for training

try:
    # Load the state dictionary
    pinn_model_loaded.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))
    pinn_model_loaded.eval() # Set model to evaluation mode
    print(f"Model loaded successfully from {MODEL_SAVE_PATH}")

    # --- Load Loss Data ---
    loss_data = np.load(LOSS_DATA_PATH)
    epochs_recorded_np = loss_data['epochs']
    losses_loaded = {
        'total': loss_data['total'],
        'pde': loss_data['pde'],
        'bc': loss_data['bc'],
        'ic': loss_data['ic']
    }
    training_time_loaded = loss_data.get('training_time', 'N/A') 
    final_lr_loaded = loss_data.get('final_lr', 'N/A')
    print(f"Loss data loaded successfully from {LOSS_DATA_PATH}")
    print(f"Training time from file: {training_time_loaded} seconds")
    print(f"Final LR from file: {final_lr_loaded}")

    # Use the loaded model and data for plotting
    pinn_model_to_plot = pinn_model_loaded
    losses_to_plot = losses_loaded
    epochs_to_plot = epochs_recorded_np

except FileNotFoundError:
    print(f"Error: Model file '{MODEL_SAVE_PATH}' or loss file '{LOSS_DATA_PATH}' not found.")
    print("Please train the model first (run Cells 1-6) or ensure the files are in the correct directory.")
    # Optionally: fall back to using the model currently in memory if training was just run
    # pinn_model_to_plot = pinn_model
    # losses_to_plot = losses
    # epochs_to_plot = epochs_recorded_np
    # print("Falling back to using model/data currently in memory (if training was just run).")


if 'epochs_to_plot' in locals() and 'losses_to_plot' in locals():
    plt.figure(figsize=(12, 7))

    # Check if lengths match before plotting
    if len(epochs_to_plot) == len(losses_to_plot['total']):
        plt.plot(epochs_to_plot, losses_to_plot['total'], label='Total Loss', linewidth=2)
        plt.plot(epochs_to_plot, losses_to_plot['pde'], label=f'PDE Loss (w={w_pde})', linestyle='--')
        plt.plot(epochs_to_plot, losses_to_plot['bc'], label=f'BC Loss (w={w_bc})', linestyle='--')
        plt.plot(epochs_to_plot, losses_to_plot['ic'], label=f'IC Loss (w={w_ic})', linestyle='--')

        plt.xlabel('Epoch')
        plt.ylabel('Loss Value (Log Scale)')
        plt.title('PINN Training Losses for Black-Scholes')
        plt.yscale('log')
        plt.legend()
        plt.grid(True, which="both", ls="-", alpha=0.5) # Add grid lines
        plt.show()
    else:
        print(f"Error plotting losses: Mismatch in length between epochs ({len(epochs_to_plot)}) and losses ({len(losses_to_plot['total'])}).")
        print("Epochs recorded:", epochs_to_plot)
        # You might want to inspect the saved npz file here if loading failed somehow

else:
    print("Please run the training cell (Cell 5) or loading cell (Cell 7) first.")

# Ensure you have loaded or trained the model first
# It expects 'pinn_model_to_plot' to be defined and loaded

if 'pinn_model_to_plot' in locals():
    pinn_model_to_plot.eval() # Ensure model is in eval mode

    # 1. Solution Comparison Plot (Surface Plots)
    n_plot = 100
    S_plot = np.linspace(S_min, S_max, n_plot)
    t_plot = np.linspace(0, T, n_plot)
    S_grid, t_grid = np.meshgrid(S_plot, t_plot)

    # Reshape for model input
    S_flat = S_grid.flatten().reshape(-1, 1)
    t_flat = t_grid.flatten().reshape(-1, 1)

    # Convert to tensor for PINN prediction
    S_tensor = torch.from_numpy(S_flat).float().to(device)
    t_tensor = torch.from_numpy(t_flat).float().to(device)

    # Get PINN predictions
    start_pred_time = time.time()
    with torch.no_grad():
        u_pred_flat = pinn_model_to_plot(S_tensor, t_tensor).cpu().numpy()
    end_pred_time = time.time()
    print(f"Prediction time for {n_plot*n_plot} points: {end_pred_time - start_pred_time:.4f} seconds")

    u_pred_grid = u_pred_flat.reshape(S_grid.shape)

    # Get Analytical solution
    u_analytical_grid = black_scholes_call_numpy(S_grid, t_grid, sigma, r, K, T)

    # Calculate Absolute Error
    abs_error_grid = np.abs(u_pred_grid - u_analytical_grid)
    # rel_error_grid = np.where(np.abs(u_analytical_grid) > 1e-6, abs_error_grid / np.abs(u_analytical_grid), 0) # Relative error (avoid division by zero)

    denominator = np.abs(u_analytical_grid)
    epsilon = 1e-8  
    safe_denominator = np.maximum(denominator, epsilon)

    raw_rel_error = abs_error_grid / safe_denominator

    rel_error_grid = np.where(denominator > epsilon, raw_rel_error, 0)

    # Plotting
    fig = plt.figure(figsize=(22, 15)) 

    # Plot 1: PINN Solution
    ax1 = fig.add_subplot(221, projection='3d')
    surf1 = ax1.plot_surface(S_grid, t_grid, u_pred_grid, cmap='viridis', edgecolor='none')
    ax1.set_title('PINN Solution $u_{\\theta}(S, t)$')
    ax1.set_xlabel('Stock Price S')
    ax1.set_ylabel('Time t')
    ax1.set_zlabel('Option Price u')
    ax1.view_init(elev=30, azim=-120) # Adjust view angle
    fig.colorbar(surf1, shrink=0.5, aspect=5, label='Price')

    # Plot 2: Analytical Solution
    ax2 = fig.add_subplot(222, projection='3d')
    surf2 = ax2.plot_surface(S_grid, t_grid, u_analytical_grid, cmap='viridis', edgecolor='none')
    ax2.set_title('Analytical Solution $u(S, t)$')
    ax2.set_xlabel('Stock Price S')
    ax2.set_ylabel('Time t')
    ax2.set_zlabel('Option Price u')
    ax2.view_init(elev=30, azim=-120)
    fig.colorbar(surf2, shrink=0.5, aspect=5, label='Price')

    # Plot 3: Absolute Error
    ax3 = fig.add_subplot(223, projection='3d')
    surf3 = ax3.plot_surface(S_grid, t_grid, abs_error_grid, cmap='plasma', edgecolor='none')
    ax3.set_title('Absolute Error $|u_{\\theta} - u|$')
    ax3.set_xlabel('Stock Price S')
    ax3.set_ylabel('Time t')
    ax3.set_zlabel('Error')
    ax3.view_init(elev=30, azim=-120)
    fig.colorbar(surf3, shrink=0.5, aspect=5, label='Abs Error')

    # Plot 4: Relative Error
    ax4 = fig.add_subplot(224, projection='3d')
    # Use log scale for relative error sometimes helps visualize, clip max error
    surf4 = ax4.plot_surface(S_grid, t_grid, np.minimum(rel_error_grid, 1.0), cmap='magma', edgecolor='none', vmax=1.0) # Cap relative error at 100% for colorbar
    ax4.set_title('Relative Error $|u_{\\theta} - u| / |u|$ (capped at 1)')
    ax4.set_xlabel('Stock Price S')
    ax4.set_ylabel('Time t')
    ax4.set_zlabel('Rel Error')
    ax4.view_init(elev=30, azim=-120)
    fig.colorbar(surf4, shrink=0.5, aspect=5, label='Rel Error')


    plt.tight_layout()
    plt.show()

    # 2. Solution Slice Plot (at specific time points)
    plt.figure(figsize=(12, 7))
    times_to_plot = [0.0, T/4.0, T/2.0, T*0.75, T] # More time slices

    for i, t_val in enumerate(times_to_plot):
        S_slice_np = np.linspace(S_min, S_max, n_plot)
        S_slice = torch.from_numpy(S_slice_np).float().reshape(-1, 1).to(device)
        t_slice = torch.full_like(S_slice, t_val)
        with torch.no_grad():
            u_pred_slice = pinn_model_to_plot(S_slice, t_slice).cpu().numpy().flatten()

        u_analytical_slice = black_scholes_call_numpy(S_slice_np, t_val, sigma, r, K, T)

        color = plt.cm.cool(i / len(times_to_plot)) # Use a colormap for slices

        # Plot analytical solution first (solid line)
        plt.plot(S_slice_np, u_analytical_slice, color=color, linestyle='-', label=f'Analytical t={t_val:.2f}')
        # Plot PINN prediction (dashed line or markers)
        plt.plot(S_slice_np, u_pred_slice, color=color, linestyle='--', label=f'PINN t={t_val:.2f}')

    plt.xlabel('Stock Price S')
    plt.ylabel('Option Price u')
    plt.title(f'PINN vs Analytical Solution at Different Times (sigma={sigma}, r={r})')
    plt.legend(fontsize='small', ncol=2) # Adjust legend
    plt.grid(True, alpha=0.5)
    plt.ylim(bottom=-5) # Allow slightly negative for visual inspection if needed
    plt.xlim(left=S_min)
    plt.show()

else:
    print("Please run the training cell (Cell 5) or loading cell (Cell 7) first.")